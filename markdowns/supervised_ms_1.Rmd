---
title: "Supervised Learning - Model Selection Part 1"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'svg')
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height =height)
})
```

So far we've created models using features that weren't necessarily the best choice. And as such we've ended up with rather lackluster models. So what are some methods to make our models better? In this topic note we will talk about how *many* features should be added. First let's do a little setup. We will create a data frame to predict IDH1 expression.

```{r df,warning=FALSE,message=FALSE}
load("lgg.rda")
library(caret)
library(limma)
#we are going to do something different here and use our gene expression data
#let's first filter, normalize and grab only the top 50 genes with the
#highest variance
filt.exp <- lgg$ExpressionData[rowMeans(lgg$ExpressionData)>10,]
log_trans <- log2(filt.exp+1)
norm_data <- normalizeQuantiles(log_trans)
vars <- rank(-apply(norm_data,1,var))<=50
#here we make sure our predictor variables are the columns
df <- as.data.frame(t(norm_data[vars,]))
df["idh1"] <- as.numeric(norm_data[grepl("IDH1",rownames(norm_data)),])
```

Now, there are metrics that inform us how many features we should add. The most infamous are the Akaike Information Criterion (AIC) and
Bayesian Information Criterion (BIC) ([Wikipedia][1],[Wikipedia][2]):

![aic](images/aic.PNG)

![bic](images/bic.PNG)

Where **k** is the number of parameters, **n** is the number of data points in the observed data, **L** is the maximum value of the likelihood function of the model. These information criterion penalize overly complex models and a lower value is typically indicative of a better model. Let's illustrate with an example.

```{r aic.bic, warning=FALSE, message=FALSE,fig.width=10,fig.height=6}
library(ggplot2)
library(viridis)
library(hrbrthemes)
library(gridExtra)
#first let's create a simple linear model
model <- lm(df$idh1~.,data = df)
#now let's loop through and get AIC/BIC values for models with different numbers
#of features. To make sure we don't get a spurious value, we will take the
#average of 50 samples at a given feature number.
AIC_vals=numeric()
for(i in 2:50){
  tmps.i <- numeric()
  for(j in 1:50){
    tmp.j <- AIC(lm(idh1~.,data = df[,c(sample(50,i),51)]))
    tmps.i[j] <- tmp.j
  }
  AIC_vals[i] = mean(tmps.i)
}
#make AIC values a data frame
AIC_vals=data.frame(
  number_of_features=1:length(AIC_vals),
  AIC=AIC_vals)
#let's do if for BIC too!
BIC_vals=numeric()
for(i in 2:50){
  tmps.i <- numeric()
  for(j in 1:50){
    tmp.j <- BIC(lm(idh1~.,data = df[,c(sample(50,i),51)]))
    tmps.i[j] <- tmp.j
  }
  BIC_vals[i] = mean(tmps.i)
}
#make AIC values a data frame
BIC_vals=data.frame(
  number_of_features=1:length(BIC_vals),
  BIC=BIC_vals)
#plot our values
p1 <- ggplot(AIC_vals, aes(x=number_of_features, y=AIC)) + 
    geom_point(color=viridis(10)[2]) +
    theme_ipsum()
p2 <- ggplot(BIC_vals, aes(x=number_of_features, y=BIC)) + 
    geom_point(color=viridis(10)[7]) +
    theme_ipsum()
grid.arrange(p1,p2,ncol=2)

```

Here we can see there are a few differences in how each should be interpreted. The point you see a rapid decrease in AIC values is the ideal number of features, while the BIC value minimum is indicative of the appropriate number of features. Here we see that the most appropriate number of features is about 10. However, we should mention that these approaches are based on likelihood functions! And some models, like neural networks and support vector machines are not based on likelihood functions. So we are restricted to using AIC/BIC on linear, logistic regression models and the like.

## Cross Validation

When talking about model selection it is worth touching on cross validation. Cross validation is an effort to increase the accuracy of a given model by splitting a data set into a training and test set ([geeksforgeeks][3]). Which we have already done! However, there are more cross validation approaches beside the validation set approach. We will then use these approaches to loop through models of different feature sizes to identify which number of features maximizes accuracy and R^2.

**Leave One Out Cross Validation** ([sthda][4])

1. Start by leaving out a data point and use the rest of the data to build a model.

2. Test the model against that one data point and capture the test error from the prediction.

3. Do this for all data points

4. The mean of these test errors is now your overall prediction error

```{r loocv, warning=FALSE,message=FALSE}
#how are we training our control
train.control <- trainControl(method = "LOOCV")
#time to train our model
model <- train(idh1~., data = df, method = "lm",
               trControl = train.control)
#our results?
print(model)
```

An even more robust method cross validation technique is k-fold cross validation:

**K-Fold Cross Validation** ([sthda][4])

1. Start by randomly splitting your data into k number groups (or folds! get it **k-fold**)

2. Put one group aside and train your model on all the other groups

3. Test this model on the group you set aside and grab the prediction error.

4. Now rinse and repeat for all k of your groups and take the average of the error values!

```{r k_fold, warning=FALSE,message=FALSE}
#how are we training our control
train.control <- trainControl(method = "cv",number = 10)
#time to train our model
model <- train(idh1~., data = df, method = "lm",
               trControl = train.control)
#our results?
print(model)
```

**Repeated K-fold Cross Validation** is the same procedure described above, just well, repeated a given number of times. What is nice about cross validation is that we can use this method on models other than linear/logistic regression models. Let's try optimizing the number of features of a linear model using k-fold cross validation!

```{r nn_cv,warning=FALSE,message=FALSE,fig.width=10,fig.height=6}
#let's create a function to do the heavy lifting.
#so we will loop through different feature sizes, create models with
#different feature sizes, k-fold cross validate those models, take the
#take the average of those
NumFeatureStats <- function(number_of_features,number_of_cv){
  rmse <- numeric()
  Rsquared <- numeric()
  for(i in 1:number_of_features){
    train.control <- trainControl(method = "cv",number = number_of_cv)
    model.df <- df[,c(sample(number_of_features,i),(number_of_features+1))]
    model <- train(idh1~., data = model.df, method = "lm",
               trControl = train.control)
    rmse <- c(rmse,mean(na.omit(model$results)$RMSE))
    Rsquared <- c(Rsquared,mean(na.omit(model$results)$Rsquared))
  }
  rmse <- data.frame(
    rmse,
    number_of_features=1:length(rmse)
  )
  Rsquared <- data.frame(
    Rsquared,
    number_of_features=1:length(Rsquared)
  )
  results <- list(rmse=rmse,Rsquared=Rsquared)
  return(results)
}
results <- NumFeatureStats(number_of_features = 50,number_of_cv = 10)
#plot our values
p1 <- ggplot(results$rmse, aes(x=number_of_features, y=rmse)) + 
    geom_point(color=magma(10)[2]) +
    theme_ipsum()
p2 <- ggplot(results$Rsquared, aes(x=number_of_features, y=Rsquared)) + 
    geom_point(color=magma(10)[7]) +
    theme_ipsum()
grid.arrange(p1,p2,ncol=2)
```

Here we can see a trade off between the accuracy of a model and how well it fits our data. And we roughly get the same picture as our AIC/BIC plots - around 10 features seems to be ideal. In the next topic note we will go over *which* features should be added.

## References

1. https://en.wikipedia.org/wiki/Akaike_information_criterion

2. https://en.wikipedia.org/wiki/Bayesian_information_criterion

3. https://www.geeksforgeeks.org/cross-validation-in-r-programming/

4. http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r
 
[1]: https://en.wikipedia.org/wiki/Akaike_information_criterion

[2]: https://en.wikipedia.org/wiki/Bayesian_information_criterion

[3]: https://www.geeksforgeeks.org/cross-validation-in-r-programming/

[4]: http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r