---
title: "Supervised Learning - Linear Regression"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'svg')
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})
```

Unlike unsupervised learning, supervised learning has a goal in mind - we are trying to predict an outcome using some model. We have already accomplished this to some degree with the linear/logistic models we have created. But let's take it a step further - let's talk about training data and test data. When we create models it is usually with the intent that we are going to predict something. When we have only one dataset, we accomplish this by splitting our data into training and test data:

![train_test](images/train_test.PNG)

So in this illustration you see that we split our data set into training and test data, create a model off the training data, then use that model to try and predict the test data. Let's try it with methods we have learned so far.

```{r data, warning=FALSE,message=FALSE}
#let's load our data 
load("./lgg.rda")
library(ggplot2)
library(limma)
#normalize data
log_trans <- log2(lgg$ExpressionData+1)
norm_data <- normalizeQuantiles(log_trans)
#grab tp53 expression and purity
tp53 <- as.numeric(norm_data[grepl("TP53",rownames(norm_data)),][!is.na(
  lgg$PatientData$paper_ABSOLUTE.purity
  )
][1,]
)
purity <- lgg$PatientData$paper_ABSOLUTE.purity[!is.na(
  lgg$PatientData$paper_ABSOLUTE.purity
  )]
#make a data frame of the two and remove NAs
df <- data.frame(
  tp53,
  purity
)
df <- na.omit(df)
#plot our data for a cursory examination
ggplot(df, aes(x=tp53, y=purity)) + 
  geom_point(color="violetred4")+
  ggtitle("")+
  annotate("text", x=13, y=0.3, label= paste("correlation: ",as.character(cor(df$tp53,df$purity)),sep = ""))
```

Here we see a weak correlation is present between TP53 expression and tumor purity. This is an encouraging sign that there is something here. Now let's split our data and build our model!

```{r pred, warning=FALSE,message=FALSE}
#split data into training and test data
train_ind <- sample(1:nrow(df), 0.8*nrow(df))
train <- df[train_ind, ]
test  <- df[-train_ind, ]
#create a model to predict purity from mutation count
model <- lm(purity~tp53,data = train)
summary(model)
#predict purity from our test data
predicted_purity <- predict(model,newdata = test)
#make df between predicted values and actual values
df_pred <- data.frame(
  test$purity,
  predicted_purity
)
#let's define a function to get root mean square error:
ModelRMSE <- function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
ModelRMSE(actual = train$purity, predicted = predict(model,train))
ModelRMSE(actual = test$purity, predicted = predict(model,test))

```

So here we split our data into test and training data, used the training data to make a model and then used that model to make predictions on our test data. Then we introduced a new term ```rmse```. This is the root mean square error, a measure of deviation between the actual values and the predicted values ([Wikipedia][1]). When both the training and test rmse are high we are typically underfitting the model ([towards data science][2]). When we have a low training rmse and a high test rmse, we have a model that is overfitting the data ([towards data science][2]). But what does under/overfitting mean? Let's use pictures ([medium][3]):

![overfitting](images/overfitting.PNG)

So here we see that underfitting isn't explaining our data enough, and overfiting explains the data too well. We need a model in somewhere in the middle. We can see from the two rmse values above, that the two rmse values are rather close indicating a more robust model.

## Bias-Variance Tradeoff

We touched on the term rmse as a metric of model accuracy and as a way to identify if a model is over or underfitting the data. Let's better understand the term rmse. The square of this, mean square error, is equal to the following ([towards data science][4]):

![error_decomp](images/error_decomp.PNG)

Here we introduce some terms - bias, variance and irreducible error. So bias and variance are the reducible error, basically we reduced the error to those terms. The irreducible error is the error we can't change by modifying our model ([towards data science][4]). Models are biased when they are too simple and oversimplify the relationship (underfitting) and have more variance if they take in too many variables and make the relationship so complex that they really just memorizing the training data (overfitting) ([towards data science][4]).

## References

1. https://en.wikipedia.org/wiki/Root-mean-square_deviation

2. https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e

3. https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76

4. https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229

[1]: https://en.wikipedia.org/wiki/Root-mean-square_deviation

[2]: https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e

[3]: https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76

[4]: https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229

[train_test]: images/train_test.PNG
[overfitting]: images/overfitting.PNG
[error_decomp]: images/error_decomp.PNG